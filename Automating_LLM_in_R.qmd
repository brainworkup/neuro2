---
title: json to yaml
---

Love that plan. Let’s move your prompts to one-prompt-per-file in YAML-front-matter Quarto. It’s infinitely nicer to read/edit than JSON, and it plays perfectly with your R/Quarto pipeline.

Below I’m giving you:
	1.	a tiny, robust converter: JSON → a folder of *.qmd prompt files
	2.	a loader that reads those *.qmd files back into R (drop-in replacement for your current JSON loader)
	3.	how to wire it so your existing generate_domain_summary_from_master() can use either source (directory of QMDs or JSON) without breaking anything

Your current JSON (e.g., the “Adaptive Functioning” item you pasted) is exactly what we’ll convert.  ￼

## 1) Converter: JSON → inst/prompts/*.qmd

```{r}
# R/convert_prompts_json_to_qmd.R
library(jsonlite)
library(fs)
library(readr)

convert_prompts_json_to_qmd <- function(
  json_path,
  out_dir = "inst/prompts",
  filename_fmt = "{keyword}.qmd",
  overwrite = FALSE
) {
  stopifnot(file.exists(json_path))
  prompts <- jsonlite::fromJSON(json_path, simplifyVector = FALSE)
  if (!is.list(prompts)) stop("Expected a JSON array of objects at: ", json_path)

  fs::dir_create(out_dir, recurse = TRUE)
  wrote <- character(0)

  for (p in prompts) {
    if (!is.list(p) || is.null(p$keyword) || is.null(p$text)) next
    kw   <- as.character(p$keyword)
    name <- as.character(p$name %||% kw)
    body <- as.character(p$text)

    # file name: priq.qmd, prsirf.qmd, etc. (strip punctuation just in case)
    safe_kw   <- gsub("[^A-Za-z0-9_-]+", "", kw)
    file_name <- gsub("{keyword}", safe_kw, filename_fmt, fixed = TRUE)
    path      <- fs::path(out_dir, file_name)

    if (fs::file_exists(path) && !overwrite) next

    # Write a Quarto file with YAML front matter + body
    # YAML front matter keeps it human-editable; body is your original prompt text
    qmd <- paste0(
      "---\n",
      "name: \"", name, "\"\n",
      "keyword: \"", kw, "\"\n",
      "---\n\n",
      body, "\n"
    )
    readr::write_file(qmd, path)
    wrote <- c(wrote, path)
  }

  invisible(wrote)
}

`%||%` <- function(x, y) if (is.null(x) || length(x) == 0) y else x
```


Run once to convert your existing JSON (working copy or the installed one—your call):

```{r}
# from your package root
convert_prompts_json_to_qmd(
  json_path = "prompt/neuro2_prompts.json",
  out_dir   = "inst/prompts",
  overwrite = TRUE
)
```


This creates files like:

```{r}
inst/prompts/
  priq.qmd
  pracad.qmd
  prverb.qmd
  ...
  prsirf.qmd
  prrecs.qmd
```

Each file has:

``` markdown
---
name: "Prompt Adaptive Functioning"
keyword: "pradapt"
---

@_02-11_adaptive_text.qmd

# Adaptive Functioning Summary Instructions
...
```

Now you can edit these directly in your IDE with syntax highlighting and normal paragraphs—no escaping, no JSON noise.

⸻

## 2) Loader: read inst/prompts/*.qmd back into R

This replaces (or complements) your read_master_prompts().

```{r}
# R/read_prompts_qmd.R
# deps: fs, yaml, readr, stringr

read_prompts_from_dir <- function(dir = system.file("prompts", package = "neuro2")) {
  if (!nzchar(dir) || !dir.exists(dir)) {
    stop("Prompts directory not found: ", dir)
  }
  files <- fs::dir_ls(dir, regexp = "\\.qmd$", type = "file")
  if (!length(files)) stop("No .qmd prompt files found in: ", dir)

  out <- lapply(files, function(f) {
    x <- readr::read_file(f)

    # Extract YAML front matter (--- ... ---) and body
    m <- stringr::str_match(x, "(?s)^\\s*---\\s*(.*?)\\s*---\\s*(.*)$")
    if (is.na(m[1,2])) {
      stop("Missing YAML front matter in: ", f)
    }
    fm   <- yaml::yaml.load(m[1,2])
    body <- m[1,3]

    if (is.null(fm$keyword) || !nzchar(fm$keyword)) {
      stop("Missing 'keyword' in YAML front matter: ", f)
    }
    if (is.null(fm$name)) fm$name <- fm$keyword

    list(
      name    = as.character(fm$name),
      keyword = as.character(fm$keyword),
      text    = as.character(body)
    )
  })

  # Keep only those that declare a target @_NN-*.qmd line (02-* and 03-*)
  has_target <- vapply(
    out,
    function(x) isTRUE(grepl("(?m)^\\s*@\\s*(_\\d{2}-[^\\s]+\\.qmd)\\s*$", x$text, perl = TRUE)),
    logical(1)
  )
  out[has_target]
}

```

⸻

## 3) Wire it into your existing functions (non-breaking)

Add prompts_dir = NULL to your main entry points; if provided, use the QMD loader; else, fall back to JSON.

In generate_domain_summary_from_master() change the signature and early loading block:

```{r}
generate_domain_summary_from_master <- function(
  master_json = NULL,
  domain_keyword,
  model = Sys.getenv("LLM_MODEL", unset = "gpt-4.1-mini"),
  temperature = 0.2,
  base_dir = ".",
  echo = "none",
  prompts_dir = NULL   # <— NEW
) {
  # Load prompts from dir if given, else from JSON
  if (!is.null(prompts_dir)) {
    prompts <- read_prompts_from_dir(prompts_dir)
  } else {
    if (is.null(master_json)) {
      master_json <- system.file("prompts", "neuro2_prompts.json", package = "neuro2")
    }
    if (!nzchar(master_json) || !file.exists(master_json)) {
      stop("Master JSON not found. Tried: ", master_json,
           "\nTip: prefer 'prompts_dir = system.file(\"prompts\", package=\"neuro2\")'")
    }
    prompts <- read_master_prompts(master_json)
  }

  # In generate_domain_summary_from_master() when selecting the prompt:
  idx <- which(vapply(
    prompts,
    function(x) .canon(x$keyword) == .canon(domain_keyword),
    logical(1)
  ))
  if (length(idx) == 0) {
    stop(
      "No prompt found in master JSON for keyword: ",
      domain_keyword,
      "\nAvailable keywords: ",
      paste0(vapply(prompts, function(x) x$keyword, ""), collapse = ", ")
    )
  }

  p <- prompts[[idx]]
  ptx <- p$text

  # 1) Detect target qmd from first @line; default to first @file if present
  target_qmd <- detect_target_qmd(ptx)
  if (is.na(target_qmd)) {
    stop("Prompt lacks a target @_02-*.qmd line for keyword: ", domain_keyword)
  }
  target_path <- file.path(base_dir, target_qmd)

  # 2) Build system + user messages
  # System prompt: the instruction block (minus chain-of-thought directives)
  sys_prompt <- sanitize_system_prompt(ptx)

  # User content = resolved includes (e.g., {{@_02-01_iq_text.qmd}}) + the target file content
  inc <- expand_includes(ptx, base_dir = base_dir)
  target_text <- read_file_or_empty(target_path)
  user_text <- paste(
    "Use the following patient/domain text to produce a single-paragraph clinical summary.",
    "Avoid test names and raw/standard/T/Scaled scores; sparingly use percentiles only if extreme.",
    "",
    "=== TARGET DOMAIN TEXT BEGIN ===",
    target_text,
    "=== TARGET DOMAIN TEXT END ===",
    "",
    "=== INCLUDED CONTEXT BEGIN ===",
    inc$text,
    "=== INCLUDED CONTEXT END ===",
    sep = "\n"
  )

  # 3) Cache key
  key <- hash_inputs(sys_prompt, user_text, deps = c(target_path, inc$deps))
  path <- file.path(llm_cache_dir(), paste0(domain_keyword, "_", key, ".txt"))
  if (file.exists(path)) {
    generated <- readr::read_file(path)
  } else {
    generated <- call_openai_once(
      system_prompt = sys_prompt,
      user_text = user_text,
      model = model,
      temperature = temperature,
      echo = echo
    )
    readr::write_file(generated, path)
  }

  # 4) Inject into the domain QMD
  inject_summary_block(target_path, generated)
  invisible(list(keyword = domain_keyword, qmd = target_path, text = generated))
}
```

In run_llm_for_all_domains() do the same:

```{r}
run_llm_for_all_domains <- function(
  master_json = NULL,
  domain_keywords = c("priq","pracad","prverb","prspt","prmem","prexe","prmot","prsoc",
                      "pradhdchild","pradhdadult","premotchild","premotadult",
                      "pradapt","prdaily","prsirf","prrecs"),
  model = Sys.getenv("LLM_MODEL", unset = "gpt-4.1-mini"),
  temperature = 0.2,
  base_dir = ".",
  echo = "none",
  prompts_dir = NULL   # <— NEW
) {
  if (!is.null(prompts_dir)) {
    prompts <- read_prompts_from_dir(prompts_dir)
  } else {
    if (is.null(master_json)) {
      master_json <- system.file("prompts", "neuro2_prompts.json", package = "neuro2")
    }
    if (!nzchar(master_json) || !file.exists(master_json)) {
      stop("Master JSON not found. Tried: ", master_json)
    }
    prompts <- read_master_prompts(master_json)
  }

  out <- lapply(domain_keywords, function(k) {
    try({
      generate_domain_summary_from_master(
        master_json = master_json,
        domain_keyword = k,
        model = model,
        temperature = temperature,
        base_dir = base_dir,
        echo = echo,
        prompts_dir = prompts_dir
      )
    }, silent = TRUE)
  })
  names(out) <- domain_keywords
  out
}

```
Now you can run everything with the nicer QMD prompts:

```{r}
# after converting JSON → QMDs and reinstalling the package…
res <- run_llm_for_all_domains(
  prompts_dir = system.file("prompts", package = "neuro2"),
  base_dir    = "."  # where your *_text.qmd live
)
```

Or during development (no reinstall) you can point to your working tree:

```{r}
res <- run_llm_for_all_domains(
  prompts_dir = "inst/prompts",
  base_dir    = "."
)
```

⸻

Why this rocks
	•	Human-friendly editing: the big prompt is just a Quarto doc; no escaping.
	•	One file per domain: easier diff/review; safer merges.
	•	Portable: when stored under inst/prompts/, system.file("prompts", package="neuro2") finds them anywhere your package is installed.
	•	Backwards-compatible: you can keep your JSON for now; both paths work.

If you want, I can also add a tiny use_neuro2_prompts() helper that returns the canonical prompts path (prefers installed inst/prompts/ if available, else your working tree) so you never pass paths manually again.
