% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_batch_openai.R
\name{call_openai_once}
\alias{call_openai_once}
\title{Call LLM Once with retries}
\usage{
call_openai_once(
  system_prompt,
  user_text,
  section = "domain",
  model_override = NULL,
  temperature = 0.2,
  echo = "none",
  max_output_tokens = NULL,
  retries = 3,
  backoff = 1
)
}
\arguments{
\item{system_prompt}{System prompt.}

\item{user_text}{User content.}

\item{section}{"domain","sirf","mega".}

\item{model_override}{Optional exact model name.}

\item{temperature}{Temperature.}

\item{echo}{Echo.}

\item{max_output_tokens}{Optional token cap (if supported by backend).}

\item{retries}{Retries.}

\item{backoff}{Initial backoff seconds.}
}
\value{
Character text output.
}
\description{
Calls the provided provider/model (Ollama by default) and returns text.
}
