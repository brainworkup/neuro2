% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_batch_openai_merged.R, R/neuro2_llm_bot.R
\name{call_openai_once}
\alias{call_openai_once}
\title{Call LLM Once with retries (Ollama backend)}
\usage{
call_openai_once(
  system_prompt,
  user_text,
  section = "domain",
  model_override = NULL,
  temperature = 0.2,
  echo = "none",
  max_output_tokens = NULL,
  retries = 3,
  backoff = 1
)

call_openai_once(
  system_prompt,
  user_text,
  section = "domain",
  model_override = NULL,
  temperature = 0.2,
  echo = "none",
  max_output_tokens = NULL,
  retries = 3,
  backoff = 1
)
}
\arguments{
\item{system_prompt}{System prompt string.}

\item{user_text}{User content string.}

\item{section}{\code{"domain"}, \code{"sirf"}, or \code{"mega"}.}

\item{model_override}{Optional exact model name (bypasses routing).}

\item{temperature}{Numeric temperature.}

\item{echo}{Echo mode for streaming.}

\item{max_output_tokens}{Optional token cap (if backend supports it).}

\item{retries}{Number of retries (default 3).}

\item{backoff}{Initial backoff seconds (default 1; doubles each retry).}
}
\value{
Character text output.

Character text output.
}
\description{
Calls the selected model and returns text; retries on transient errors.

Calls the selected model and returns text; retries on transient errors.
}
